{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 3: COVID-19 Data Analysis and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Created by Group 7 - Kamilla, Jeanette, Juvena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment aims to develop practical skills in data analysis, visualization, and machine learning using real-world COVID-19 data. The project focuses on exploring global pandemic-related indicators to uncover trends, build predictive models, and apply both supervised and unsupervised learning techniques using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin analyzing the COVID-19 dataset, we need to import a few essential Python libraries that will help us manipulate the data, build models, and visualize our findings:\n",
    "\n",
    "- **Pandas**: This is a powerful library used to handle and manipulate data in tables (called DataFrames).\n",
    "- **NumPy**: It helps with numerical operations, especially when we work with arrays or need to do math.\n",
    "- **Matplotlib** and **Seaborn**: These are popular libraries for creating visual charts and graphs. We'll use them to help us understand the data better by seeing it.\n",
    "- **SciPy (stats module)**: This gives us access to statistical tools like checking if data is normally distributed.\n",
    "\n",
    "We'll also configure default styles for our plots to ensure they're clean, visually appealing, and easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sm\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set plot styles for better visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data wrangling and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tools ready, the next step is to load the COVID-19 dataset into Python so we can start analyzing it.\n",
    "\n",
    "In this case, we’re working with a single dataset:\n",
    "\n",
    "- **OWID COVID-19 Latest Data**: a CSV file that contains country-level information on cases, deaths, vaccinations, testing, and various socioeconomic indicators.\n",
    "\n",
    "We'll use Pandas to read the CSV file and store it as a DataFrame. To make our code cleaner and reusable, we'll define a simple function that loads the data and performs some initial checks. This way, we can easily reload or replace the dataset if needed in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the covid datasets. (dataset: last updated 2024-08-04)\n",
    "dataset_covid = 'Dataset/owid-covid-latest.csv'\n",
    "\n",
    "# Function to load the Excel files\n",
    "def load_csv_to_dataframe(file_path):\n",
    "    # Reads the Excel file and skips the first row if it contains a description or title\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Load datasets\n",
    "print(\"..Loading COVID-19 dataset\")\n",
    "df_covid = load_csv_to_dataframe(dataset_covid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the dataset, we want to explore it to understand what kind of information it contains and how it's structured.\n",
    "\n",
    "To do this, we can use several helpful Pandas functions such as `shape`, `types`, `info()`, `head()`, `tail()`, `sample()`, `describe()` and `isnull().sum()`. These functions will give us insights into the number of rows and columns, the data types of each column, a summary of the data, and any missing values. \n",
    "\n",
    "This exploration is crucial as it helps us identify potential issues or areas that need further cleaning or transformation before we proceed with our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame (rows, columns)\n",
    "df_covid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the types of attributes (colum names) in the DataFrame\n",
    "df_covid.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives an overview of the DataFrame\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the DataFrame\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 5 rows of the DataFrame\n",
    "df_covid.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 5 rows from the DataFrame\n",
    "df_covid.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives summary statistics for all numerical columns in the dataset\n",
    "df_covid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.2.1 Summary of exploring the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the dataframe, we found that it contains a large number of columns, many of which are not useful for our analysis or modeling goals. While some columns provide valuable information (like total cases, deaths, and vaccination rates), others are either redundant, mostly empty, or irrelevant.\n",
    "\n",
    "This highlights the need for a thorough data cleaning step to remove unnecessary columns, handle missing values, and focus only on the most relevant features for our machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading and exploring the data, we need to clean it to ensure that our analysis is accurate and meaningful. Data cleaning involves several steps, including: checking for missing values, removing duplicates, and converting data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "df_covid.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that many columns contain no values at all, so we will remove them to clean up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before cleaning the data, we want to remove irrelevant OWID aggregate rows—such as those representing high-income, low-income, and other income groupings.\n",
    "rows_to_remove = [\"OWID_UMC\", \"OWID_WRL\", \"OWID_LMC\", \"OWID_LIC\", \"OWID_HIC\"]\n",
    "df_removed_rows = df_covid[~df_covid[\"iso_code\"].isin(rows_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are removing the 'low-income countries', 'lower-middle-income countries', 'upper-middle-income countries', 'high-income countries' and 'world' categories because they are too broad and lack specific country-level detail, making it difficult to draw meaningful conclusions without relying on assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the above rows were removed\n",
    "print(f\"{df_covid.shape}\")\n",
    "print(f\"Removed the {df_covid.shape[0] - df_removed_rows.shape[0]} OWID rows from the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will drop all columns with no values at all like; excess_mortality_cumulative_absolute, excess_mortality_cumulative etc.\n",
    "df_covid_removed_columns = df_removed_rows.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the columns were removed\n",
    "print(f\"COVID dataframe shape after removing columns: {df_covid_removed_columns.shape}\")\n",
    "print(f\"Removed {df_covid.shape[1] - df_covid_removed_columns.shape[1]} columns from the dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected a subset of columns that we consider relevant for our analysis. These columns include key features related to COVID-19 cases, deaths, vaccinations, demographics, and health indicators. By keeping only these columns, we focus on the most informative data for building meaningful models and drawing accurate insights, while reducing noise and unnecessary complexity in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a new dataframe with the columns we want to keep for future analysis.\n",
    "columns_we_want_to_keep = [\n",
    "    \"iso_code\", \"continent\", \"location\", \"total_cases\", \"total_deaths\",\n",
    "    \"total_cases_per_million\", \"total_deaths_per_million\",\n",
    "    \"total_vaccinations\", \"people_vaccinated\", \"people_fully_vaccinated\",\n",
    "    \"total_boosters\", \"new_vaccinations\", \"new_vaccinations_smoothed\",\n",
    "    \"total_vaccinations_per_hundred\", \"people_vaccinated_per_hundred\",\n",
    "    \"people_fully_vaccinated_per_hundred\", \"total_boosters_per_hundred\",\n",
    "    \"new_vaccinations_smoothed_per_million\", \"new_people_vaccinated_smoothed\",\n",
    "    \"new_people_vaccinated_smoothed_per_hundred\", \"population_density\",\n",
    "    \"median_age\", \"aged_65_older\", \"aged_70_older\", \"cardiovasc_death_rate\",\n",
    "    \"diabetes_prevalence\", \"female_smokers\", \"male_smokers\",\n",
    "    \"life_expectancy\", \"population\"\n",
    "]\n",
    "\n",
    "# Removes all other columns\n",
    "df_covid = df_covid_removed_columns[columns_we_want_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the columns were removed\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the dataset look and how we should proceed\n",
    "df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataframe after some cleaning\n",
    "print(f\"COVID dataframe shape after removing both some columns and rows: {df_covid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Separating the continent-level data into its own DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are separating the continent-level data into its own DataFrame so that we can clean and process it independently from the country-level data. This allows us to apply different cleaning steps based on the nature of the data, since continent aggregates may have different structures or missing values compared to individual countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before removing the iso_code column, we want to secure the OWID fields for the continents since it could be relevant data to analyze.\n",
    "rows_to_secure = [\"OWID_AFR\", \"OWID_ASI\", \"OWID_EUR\", \"OWID_EUN\", \"OWID_NAM\", \"OWID_OCE\", \"OWID_SAM\"]\n",
    "df_continents = df_covid[df_covid[\"iso_code\"].isin(rows_to_secure)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the rows were secured\n",
    "df_continents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a new seperate dataframe called `df_continent` that contains the continent-level data. This DataFrame will be used for further analysis and modeling, while the original `df_covid` DataFrame will focus on country-level data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns that are irrelavnt since they containt no values at all like; population_density, median_age\n",
    "df_continents_romved_columns = df_continents.dropna(axis=1, how='all')\n",
    "df_continents_romved_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the columns were removed\n",
    "print(f\"df_continent shape after removing columns: {df_continents_romved_columns.shape}\")\n",
    "print(f\"Removed {df_continents.shape[1] - df_continents_romved_columns.shape[1]} columns from the dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns `new_vaccinations_smoothed_per_million`, `new_people_vaccinated_smoothed` and `new_people_vaccinated_smoothed_per_hundred` contain a lot of missing values so they are not necessary for our analysis and we will drop them from the `df_continents_cleaned` DataFrame. By removing them, we can simplify the DataFrame and focus on the most relevant features for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns there are irrelevant \n",
    "df_continents_cleaned = df_continents_romved_columns.drop(['iso_code', 'new_vaccinations_smoothed_per_million', 'new_people_vaccinated_smoothed', 'new_people_vaccinated_smoothed_per_hundred'], axis=1)\n",
    "df_continents_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the columns were removed\n",
    "print(f\"df_continent shape after removing columns: {df_continents_cleaned.shape}\")\n",
    "print(f\"Removed {df_continents_romved_columns.shape[1] - df_continents_cleaned.shape[1]} columns from the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the DataFrame\n",
    "df_continents_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some rows with missing values in the df_continents_cleaned DataFrame, so we will impute them to ensure that our analysis is accurate and meaningful. This step is important because missing values can lead to biased results or errors in our models.\n",
    "\n",
    "Since it is a small dataframe, we can't delete the row with missing values(NaN), since we will lose a lot of data. We choose to replace the missing values, even though it can have a high risk of giving wrong information and have a big impact. \n",
    "\n",
    "The first four we replaced using realistic data.\n",
    "But since it took too much time, we’ll use the median to fill the remaining NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method for replacing cell with a value\n",
    "def replace_cell(df, row_filter, column, value):\n",
    "    df.loc[row_filter, column] = value\n",
    "\n",
    "# replace NaN for total_vaccinations for Africa. 1.084.500.000 is from Africa CDC, which is offical and reliable.\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'Africa', 'total_vaccinations', 1084500000)\n",
    "\n",
    "# replace NaN for total_vaccinations for South America. 970.800.000 is from WTO-IMF COVID-19 Vaccine Trade Tracker, which is offical and reliable.\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'South America', 'total_vaccinations', 970800000)\n",
    "\n",
    "# replace NaN for people_vaccinated for Africa. 725.000.000 is from Africa CDC, which is offical and reliable\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'Africa', 'people_vaccinated', 725000000)\n",
    "\n",
    "# replace NaN for people_vaccinated for South America. 351.310.000 is from Our World in Data which is offical and reliable used by WHO.\n",
    "replace_cell(df_continents_cleaned, df_continents_cleaned['location'] == 'South America', 'people_vaccinated', 351310000)\n",
    "\n",
    "# method for replacing cell with median \n",
    "def fill_na_with_median(df, column_name):\n",
    "    median_value = df[column_name].median()\n",
    "    print(f\"Median of '{column_name}': {median_value:.2f}\")\n",
    "    df[column_name].fillna(median_value, inplace=True)\n",
    "\n",
    "\n",
    "fill_na_with_median(df_continents_cleaned, 'people_fully_vaccinated')\n",
    "fill_na_with_median(df_continents_cleaned, 'total_boosters')\n",
    "fill_na_with_median(df_continents_cleaned, 'new_vaccinations')\n",
    "fill_na_with_median(df_continents_cleaned, 'new_vaccinations_smoothed')\n",
    "fill_na_with_median(df_continents_cleaned, 'total_vaccinations_per_hundred')\n",
    "fill_na_with_median(df_continents_cleaned, 'people_vaccinated_per_hundred')\n",
    "fill_na_with_median(df_continents_cleaned, 'people_fully_vaccinated_per_hundred')\n",
    "fill_na_with_median(df_continents_cleaned, 'total_boosters_per_hundred')\n",
    "df_continents_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Isolating the remaining rows in the `df_covid` dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are isolating the remaining rows in the df_covid DataFrame to ensure it contains only country-level data. This allows us to clean the dataset and retain only the features that are most relevant for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we seperated the OWID continent fields into it's own dataframe earlier, we now have to remove them again for the df_covid dataframe.\n",
    "rows_to_remove = [\"OWID_AFR\", \"OWID_ASI\", \"OWID_EUR\", \"OWID_EUN\", \"OWID_NAM\", \"OWID_OCE\", \"OWID_SAM\"]\n",
    "df_covid_removed_rows = df_covid[~df_covid['iso_code'].isin(rows_to_remove)]\n",
    "df_covid_removed_rows             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the rows were removed\n",
    "print(f\"COVID dataframe shape after removing columns: {df_covid_removed_rows.shape}\")\n",
    "print(f\"Removed {df_covid.shape[0] - df_covid_removed_rows.shape[0]} columns from the dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns such as `iso_code`, `total_vaccinations`, `population_density`, `aged_70_older`, `female_smokers` and others either contain no values or have a high number of missing entries. Since our analysis focuses on deaths and infections in relation to population, these columns are not essential. Therefore, we will remove them from the `df_covid_removed_rows` DataFrame to simplify the dataset and concentrate on the most relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns there are irrelavnt from df_covid_cleaned\n",
    "df_covid_cleaned = df_covid_removed_rows.drop([\n",
    "    \"iso_code\",\"total_vaccinations\", \"people_vaccinated\", \"people_fully_vaccinated\",\n",
    "    \"total_boosters\", \"new_vaccinations\", \"new_vaccinations_smoothed\",\n",
    "    \"total_vaccinations_per_hundred\", \"people_vaccinated_per_hundred\",\n",
    "    \"people_fully_vaccinated_per_hundred\", \"total_boosters_per_hundred\",\n",
    "    \"new_vaccinations_smoothed_per_million\", \"new_people_vaccinated_smoothed\",\n",
    "    \"new_people_vaccinated_smoothed_per_hundred\", \"population_density\", \"median_age\", \n",
    "    \"aged_65_older\", \"aged_70_older\", \"cardiovasc_death_rate\", \"diabetes_prevalence\", \n",
    "    \"female_smokers\", \"male_smokers\"], axis=1)\n",
    "df_covid_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the columns were removed\n",
    "print(f\"COVID dataframe shape after removing columns: {df_covid_cleaned.shape}\")\n",
    "print(f\"Removed {df_covid_removed_rows.shape[1] - df_covid_cleaned.shape[1]} columns from the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in the DataFrame\n",
    "df_covid_cleaned.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have some rows with missing values in the `df_covid_cleaned` DataFrame, so we will impute them with median to ensure that our analysis is accurate and meaningful. This step is important because missing values can lead to biased results or errors in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fill NaN values with the median of the specified column\n",
    "def fill_na_with_median(df, column_name):\n",
    "    median_value = df[column_name].median()\n",
    "    print(f\"Median of {column_name}: {median_value:.2f}\")\n",
    "    df[column_name].fillna(median_value, inplace=True)\n",
    "\n",
    "# Fill NaN values with the median for the columns; total_cases, total_deaths, total_cases_per_million, total_deaths_per_million and life_expectancy\n",
    "fill_na_with_median(df_covid_cleaned, \"total_cases\")\n",
    "fill_na_with_median(df_covid_cleaned, \"total_deaths\")\n",
    "fill_na_with_median(df_covid_cleaned, \"total_cases_per_million\")\n",
    "fill_na_with_median(df_covid_cleaned, \"total_deaths_per_million\")\n",
    "fill_na_with_median(df_covid_cleaned, \"life_expectancy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Explore the new cleaned dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Explore df_continents_cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame (rows, columns)\n",
    "df_continents_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives an overview of the DataFrame\n",
    "df_continents_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns in the DataFrame\n",
    "list(df_continents_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives summary statistics for all numerical columns in the dataset\n",
    "df_continents_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.1.1 Check for outliers in the df_continents_cleaned**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in exploring the data is checking for outlier values that are unusually high or low compared to the rest of the data.\n",
    "\n",
    "We use the IQR (Interquartile Range) method, which is a common way to detect outliers:\n",
    "\n",
    "-  First, we calculate the first quartile (Q1) and third quartile (Q3) for each selected column.\n",
    "- The IQR is the difference between Q3 and Q1.\n",
    "- Any value that falls below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR is considered an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continents_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in df_continents_cleaned using IQR method\n",
    "print(\"\\n..Checking for outliers in df_continents_cleaned:\")\n",
    "\n",
    "# Loop through selected columns\n",
    "for column in ['total_cases_per_million', 'total_deaths_per_million', 'people_vaccinated_per_hundred']:\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df_continents_cleaned[column].quantile(0.25)\n",
    "    Q3 = df_continents_cleaned[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "\n",
    "    # Define the lower and upper bounds for detecting outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Find rows where the value is outside the normal range\n",
    "    outliers = df_continents_cleaned[\n",
    "        (df_continents_cleaned[column] < lower_bound) | \n",
    "        (df_continents_cleaned[column] > upper_bound)\n",
    "    ]\n",
    "\n",
    "    # Print the number of outliers found for the column\n",
    "    print(f\"  {column}: {len(outliers)} outliers detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the outlier there was detected\n",
    "print(outliers[['location', column]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.1.2 Conclusion of outliers**:\n",
    "Only one outlier was detected in the people_vaccinated_per_hundred column.\n",
    "This indicates that one continent(Oceania) slightly deviates in terms of vaccination coverage compared to the others.\n",
    "However the value is not far from the overall range, so it will be kept in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.1.3 Visualize the Dataset Statistics - df_continents_cleaned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms for df_continents_cleaned\n",
    "columns = ['total_cases_per_million', 'total_deaths_per_million', 'people_vaccinated_per_hundred']\n",
    "axes = df_continents_cleaned[columns].hist(figsize=(10, 5))\n",
    "for ax, col in zip(axes.flatten(), columns):\n",
    "    ax.set_xlabel(col + \" (value)\")         \n",
    "    ax.set_ylabel(\"number of continents\") \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above histogram shows following:\n",
    "\n",
    "Total cases per million:\n",
    "Most continents have between 300,000 and 400,000 cases per million.\n",
    "\n",
    "Total deaths per million:\n",
    "The death rate is higher in a few continents, suggesting potential differences in healthcare or reporting.\n",
    "\n",
    "People vaccinated per hundred:\n",
    "Most continents have similar vaccination coverage, except Oceania, which is lower and may indicate slower rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart for df_continents_cleaned\n",
    "sns.barplot(x='location', y='total_deaths_per_million', data=df_continents_cleaned)\n",
    "plt.title('Total COVID-19 deaths per million by continent')\n",
    "plt.xlabel('Continent')\n",
    "plt.ylabel('Deaths per million')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above bar chart shows there are clear differences in COVID-19 deaths per million across continents. \n",
    "South America has the highest death rate, followed closely by Europe and North America. \n",
    "In contrast, Africa and Asia show much lower death rates, which may reflect differences in healthcare systems, \n",
    "demographics, or data reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Explore df_covid_cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame (rows, columns)\n",
    "df_covid_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives an overview of the DataFrame\n",
    "df_covid_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a random sample of 10 rows from the DataFrame\n",
    "df_covid_cleaned.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns in the DataFrame\n",
    "list(df_covid_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives summary statistics for all numerical columns in the dataset\n",
    "df_covid_cleaned.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we explored the new cleaned dataframe a bit, we can see that the `df_covid_cleaned` DataFrame contains a more manageable number of columns and rows vs the original dataframe. The columns we have retained are relevant for our analysis, and we have removed unnecessary or redundant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.2.1 Check for outliers in the df_covid_cleaned**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this method to four important features in both datasets: total_cases, total_deaths, life_expectancy and population. This helps us find any unusual data points that could affect the results of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in covid dataset using IQR method\n",
    "print(\"\\n..Checking for outliers in the covid dataframe:\")\n",
    "\n",
    "# Loop through selected columns\n",
    "for column in ['total_cases', 'life_expectancy', 'population']:\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df_covid_cleaned[column].quantile(0.25)\n",
    "    Q3 = df_covid_cleaned[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1  # Interquartile Range\n",
    "\n",
    "    # Define the lower and upper bounds for detecting outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Find rows where the value is outside the normal range\n",
    "    outliers = df_covid_cleaned[\n",
    "        (df_covid_cleaned[column] < lower_bound) | \n",
    "        (df_covid_cleaned[column] > upper_bound)\n",
    "    ]\n",
    "\n",
    "    # Print the number of outliers found for the column\n",
    "    print(f\"  {column}: {len(outliers)} outliers detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.2.2 Conclusion of outliers**: The dataset contains outliers across several features — particularly in total_cases (39 outliers) and population (25 outliers).\n",
    "\n",
    "These outliers are likely not errors but reflect extreme yet valid data points related to the real impact of COVID-19 in certain countries. For this reason, we’ve chosen to keep them. These values could provide valuable insights into the factors that influenced high case and death counts. Removing them might hide important patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **1.4.2.3 Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get statistics\n",
    "scaled_data = df_covid_cleaned[['total_deaths_per_million']]\n",
    "\n",
    "print('Mean:', scaled_data['total_deaths_per_million'].mean())\n",
    "print('Standard Deviation:', scaled_data['total_deaths_per_million'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw histogram to visualize them\n",
    "sns.histplot(scaled_data['total_deaths_per_million'], color='#ee4c2c', bins=50);\n",
    "plt.ylabel(\"Number of countries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the graph above shows, most countries have fewer than 1000 deaths per million, but a few have significantly higher numbers. This creates a right-skewed distribution, possibly reflecting differences in healthcare systems or pandemic responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.2.4 Standard Scalling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce all with the mean and scale the data to unit variance\n",
    "# x = (x-xmean)/std\n",
    "standard_scaler = StandardScaler()\n",
    "scaled_data['total_deaths_per_million'] = standard_scaler.fit_transform(scaled_data[['total_deaths_per_million']])\n",
    "\n",
    "print('Mean:', scaled_data['total_deaths_per_million'].mean()) # almost 0\n",
    "print('Standard Deviation:', scaled_data['total_deaths_per_million'].std()) # almost 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# histogram has same shape, but 0,0 is in the middle\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.histplot(scaled_data['total_deaths_per_million'], color='#ee4c2c', bins=50);\n",
    "plt.ylabel(\"Number of countries\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standard scaling, the distribution shape remains the same, but values are now centered around 0 with a standard deviation of 1. This makes the data easier to compare with other features and is especially useful for machine learning models that are sensitive to different value ranges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.2.5 Min-Max Scalling - Normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "scaled_data['death_min_max_scaled'] = minmax_scaler.fit_transform(scaled_data[['total_deaths_per_million']])\n",
    "\n",
    "print('Mean:', scaled_data['death_min_max_scaled'].mean())\n",
    "print('Standard Deviation:', scaled_data['death_min_max_scaled'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values are in [0, 1]\n",
    "sns.histplot(scaled_data['death_min_max_scaled'], color='#ee4c2c', bins=50);\n",
    "plt.ylabel(\"Number of countries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrans = QuantileTransformer()\n",
    "scaled_data['death_trans_uniform'] = qtrans.fit_transform(scaled_data[['total_deaths_per_million']])\n",
    "\n",
    "print('Mean:', scaled_data['death_trans_uniform'].mean())\n",
    "print('Standard Deviation:', scaled_data['death_trans_uniform'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrans = QuantileTransformer()\n",
    "scaled_data['death_trans_uniform'] = qtrans.fit_transform(scaled_data[['total_deaths_per_million']])\n",
    "\n",
    "print('Mean:', scaled_data['death_trans_uniform'].mean())\n",
    "print('Standard Deviation:', scaled_data['death_trans_uniform'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='continent', y='total_deaths_per_million', data=df_covid_cleaned)\n",
    "plt.title(\"Total deaths per million by continent\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above boxplot shows that South America and Europe have the highest median deaths per million, while Africa and Asia have the lowest. There are many outliers, especially in Africa, indicating large variation between countries within continents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.2.6 Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_covid_cleaned.select_dtypes(include='number').corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation matrix over COVID-19 and related factors\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a weak correlation (−0.07) between population size and total deaths per million, suggesting that population alone does not explain differences in COVID-19 death rates. Other factors like healthcare quality or pandemic response may play a larger role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.2.7 Checking for Normal Distribution for the df_covid_cleaned**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for Normal Distribution\n",
    "Before applying statistical methods, it is important to understand how the data is distributed. Many techniques — such as t-tests, ANOVA, and linear regression — assume that the data follows a normal distribution. If this assumption is not met, the results may not be reliable.\n",
    "\n",
    "To check for normality, we define a custom function called check_normality. This function runs through each numeric column (excluding total_deaths) and performs several checks:\n",
    "\n",
    "**Skewness** measures how symmetric the data is:\n",
    "\n",
    "- A positive skew means the distribution has a long tail on the right.\n",
    "- A negative skew means the tail is on the left.\n",
    "\n",
    "General interpretation:\n",
    "\n",
    "- |skew| < 0.5 → roughly symmetric\n",
    "- 0.5 < |skew| < 1 → moderately skewed\n",
    "- |skew| > 1 → highly skewed\n",
    "\n",
    "**Kurtosis** measures how sharp or flat the peak is, and how heavy the tails are compared to a normal curve:\n",
    "\n",
    "- Kurtosis > 3 → sharper peak, more outliers\n",
    "- Kurtosis < 3 → flatter peak, fewer outliers\n",
    "- In pandas, the `kurt()` function returns **excess kurtosis**, so a value of 0 indicates a normal distribution.\n",
    "\n",
    "The function also applies two statistical tests for normality:\n",
    "\n",
    "- D’Agostino’s K-squared test\n",
    "- Jarque-Bera test\n",
    "\n",
    "Both tests return a p-value. If the p-value is greater than 0.05, the data is considered likely to be normally distributed. If the p-value is below 0.05, the data likely deviates from a normal distribution.\n",
    "\n",
    "Based on these results, each feature is classified as:\n",
    "\n",
    "- Yes – likely normal (both p-values > 0.05 and skewness < 1)\n",
    "- Partial – somewhat normal (D’Agostino p > 0.01 and skewness < 2)\n",
    "- No – not normal (does not meet the above criteria)\n",
    "\n",
    "The function returns a summary table showing each feature’s skewness, kurtosis, p-values, and classification. This step is an important part of exploratory data analysis and helps us decide whether we need to transform any variables or choose alternative statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n..Test normality in the covid dataframe:\")\n",
    "\n",
    "# Function to test normality of numeric columns\n",
    "def check_normality(df):\n",
    "    \"\"\"\n",
    "    Tests whether numeric columns follow a normal distribution.\n",
    "    Uses D’Agostino and Jarque-Bera tests. \n",
    "    \"\"\"\n",
    "    num_cols = [col for col in df.select_dtypes(include=['float64', 'int64']).columns \n",
    "                if col != 'total_deaths']\n",
    "    \n",
    "    rows = []\n",
    "\n",
    "    for col in num_cols:\n",
    "        data = df[col]\n",
    "        skewness = data.skew()\n",
    "        kurtosis = data.kurt()\n",
    "        dagostino = stats.normaltest(data)\n",
    "        jb = stats.jarque_bera(data)\n",
    "\n",
    "        normal = \"No\"\n",
    "        if dagostino.pvalue > 0.05 and jb.pvalue > 0.05 and abs(skewness) < 1:\n",
    "            normal = \"Yes\"\n",
    "        elif dagostino.pvalue > 0.01 and abs(skewness) < 2:\n",
    "            normal = \"Partial\"\n",
    "\n",
    "        rows.append({\n",
    "            'Column': col,\n",
    "            'Skewness': round(skewness, 3),\n",
    "            'Kurtosis': round(kurtosis, 3),\n",
    "            \"D'Agostino p-value\": f\"{dagostino.pvalue:.2e}\",\n",
    "            \"Jarque-Bera p-value\": f\"{jb.pvalue:.2e}\",\n",
    "            'Normally Distributed?': normal\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Run normality checks on all numeric columns\n",
    "check_normality(df_covid_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distributions(df):\n",
    "    \"\"\"\n",
    "    Visualizes the distribution of all numeric columns with histogram and QQ-plot.\n",
    "    \"\"\"\n",
    "    num_cols = [col for col in df.select_dtypes(include=['float64', 'int64']).columns \n",
    "                if col != 'total_deaths']\n",
    "    \n",
    "    fig, axes = plt.subplots(len(num_cols), 2, figsize=(12, 4 * len(num_cols)))\n",
    "\n",
    "    for i, col in enumerate(num_cols):\n",
    "        ax1 = axes[i, 0] if len(num_cols) > 1 else axes[0]\n",
    "        ax2 = axes[i, 1] if len(num_cols) > 1 else axes[1]\n",
    "\n",
    "        sns.histplot(df[col], kde=True, ax=ax1)\n",
    "        ax1.set_title(f'Distribution of {col}')\n",
    "\n",
    "        stats.probplot(df[col], dist=\"norm\", plot=ax2)\n",
    "        ax2.set_title(f'QQ-Plot of {col}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function\n",
    "visualize_distributions(df_covid_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.2.8 Summary: Normality of Numeric Variables**\n",
    "\n",
    "Statistical tests and visualizations show that none of the numeric variables in the dataset are normally distributed. Variables like `total_cases` and `population` are highly right-skewed, while others such as `total_cases_per_million`, `total_deaths_per_million`, and `life_expectancy` show moderate skewness but still fail normality tests.\n",
    "\n",
    "With a moderate sample size of 235 observations, these results likely reflect genuine distribution patterns rather than test sensitivity. This indicates that analytical methods assuming normality (e.g., parametric tests or linear regression without transformation) may not be suitable without preprocessing steps like normalization or transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.4.2.9 Feature Selection**\n",
    "Feature selection and comparison were carried out during the data cleaning and exploratory process. In our analysis, we consider deaths as the dependent variable, while the remaining features are treated as independent variables that may help explain or influence the death toll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Supervised machine learning: linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualise the features and the response using scatterplots\n",
    "sns.pairplot(df_covid_cleaned, x_vars=['total_cases_per_million'], y_vars='total_deaths_per_million', height=5, aspect=1.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent\n",
    "X = df_covid_cleaned['total_cases_per_million'].values.reshape(-1, 1)\n",
    "# dependent\n",
    "y = df_covid_cleaned['total_deaths_per_million'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all\n",
    "plt.ylabel('total_deaths_per_million')\n",
    "plt.xlabel('total_cases_per_million')\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot shows a weak positive correlation between total COVID-19 cases per million and deaths per million. However, the data is widely spread with several outliers, which reduces the accuracy of a linear regression model. This suggests that the relationship is not strongly linear and may require further data processing or additional features for better prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_cleaned.plot.line(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='total_cases_per_million',y='total_deaths_per_million',data=df_covid_cleaned,fit_reg=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot with regression line shows a weak positive relationship between total COVID-19 cases and deaths per million. Although the trend is upward, the wide confidence interval and spread of points indicate high variability, suggesting the linear model has limited predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of the subsets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance of Linear Regression model\n",
    "myreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit it to our data\n",
    "myreg.fit(X_train, y_train)\n",
    "myreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calculated coefficients\n",
    "a = myreg.coef_\n",
    "b = myreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The model is a line, y = a * x + b, or y = {a} * x + {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = myreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the Linear Regression \n",
    "plt.title('Linear Regression')\n",
    "plt.scatter(X, y, color='green')\n",
    "plt.plot(X_train, a*X_train + b, color='blue')\n",
    "plt.plot(X_test, y_predicted, color='orange')\n",
    "plt.xlabel('total_cases_per_million')\n",
    "plt.ylabel('total_deaths_per_million')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression plot shows a weak positive correlation between COVID-19 cases and deaths per million. The blue line represents the model's fitted trend, while orange dots indicate predictions. The high spread of actual values around the line suggests limited predictive accuracy of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict age from length\n",
    "death_predicted = myreg.predict([[170]])\n",
    "death_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_predict = a * 170 + b\n",
    "death_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unknown data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 145\n",
    "death_predicted = myreg.predict([[length]])\n",
    "death_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's predictive power is limited, it can still be used to estimate the expected number of deaths per million based on new case data. For example, given 145,000 cases per million, the model predicts approximately 724 deaths per million. However, due to high variance in the data, this should only be interpreted as a rough estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "mae = metrics.mean_absolute_error(y_test, y_predicted)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "mse = metrics.mean_squared_error(y_test, y_predicted)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_predicted))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE is the easiest to understand, because it's the average error, measured in the same units like the data\n",
    "MSE is more popular than MAE, because MSE amplifies larger errors, making it useful when larger errors are particularly costly\n",
    "RMSE is even more popular than MSE, because RMSE combines the benefits of both MSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance score: the proportion of the variance in a dependent variable that can be explained by the model\n",
    "# 1 for perfect prediction\n",
    "eV = round(sm.explained_variance_score(y_test, y_predicted), 2)\n",
    "print('Explained variance score ',eV )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared: the proportion of the variation in the dependent variable that is predictable from the independent variable(s)\n",
    "#r2_score(y, predict(X))\n",
    "#r2_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Supervised machine learning: classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate what factors may indicate a country’s risk of experiencing a high number of COVID-19 deaths, we trained a Random Forest Classifier using total cases, life expectancy, and population as input features. A binary target variable, high_mortality, was created by labeling countries with total deaths above the median as \"high mortality\" and those below as \"low mortality\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary target based on the median\n",
    "median_deaths = df_covid_cleaned['total_deaths'].median()\n",
    "df_covid_cleaned['high_mortality'] = df_covid_cleaned['total_deaths'] > median_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose feature and target \n",
    "features = ['total_cases', 'life_expectancy', 'population']\n",
    "target = 'high_mortality'\n",
    "\n",
    "X = df_covid_cleaned[features]\n",
    "y = df_covid_cleaned[target]\n",
    "\n",
    "# Split i træning og test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested the model using a 75/25 split between training and test data. The model achieved an accuracy of 88.1%, indicating high precision and strong generalization ability. A previous test using an 80/20 split gave 87.2%, confirming the model’s stability across different data splits. Precision will be 0.85 (low mortality), 0.91 (high mortality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and sort the feature importance \n",
    "importances = model.feature_importances_\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=features_df, x='Importance', y='Feature')\n",
    "plt.title('Feature Importance (Random Forest Classifier)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unsupervised machine learning: clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To uncover patterns in how countries were affected by COVID-19, we applied KMeans clustering using the features:\n",
    "total_cases, life_expectancy, and population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose numeric features for clustering\n",
    "features_cluster = ['total_cases', 'life_expectancy', 'population']\n",
    "X_cluster = df_covid_cleaned[features_cluster]\n",
    "\n",
    "# Standardise data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans cluster\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# add cluster labels for dataframe\n",
    "df_covid_cleaned['cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette score\n",
    "score = silhouette_score(X_scaled, clusters)\n",
    "print(f\"Silhouette Score: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before clustering, we standardized the data using StandardScaler to ensure equal weighting of features.\n",
    "\n",
    "We chose to use KMeans with 3 clusters, which grouped countries based on similarities in the selected variables. The silhouette score was 0.53, indicating a reasonably good separation between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    x='total_cases_per_million',\n",
    "    y='life_expectancy',\n",
    "    hue='cluster',\n",
    "    data=df_covid_cleaned,\n",
    "    palette='Set2'\n",
    ")\n",
    "plt.title('KMeans Clustering of Countries')\n",
    "plt.xlabel('Cases per Million')\n",
    "plt.ylabel('Life Expectancy')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters were visualized in a 2D scatterplot showing how countries separate based on life expectancy and COVID-19 cases. The results showed:\n",
    "Cluster 1 (168 countries): moderate life expectancy and high case numbers\n",
    "Cluster 2 (64 countries): generally lower life expectancy and lower case numbers\n",
    "Cluster 0 (3 countries): outliers with extreme combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "k_values = range(1, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(X_scaled)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of our MP3 project, we realized that we need to gather more data for our exam project to better address our hypotheses about factors influencing COVID-19 death rates. However, based on our current analysis — including the correlation matrix — we can conclude that there is a weak correlation (−0.07) between population size and total deaths per million. This suggests that population alone does not explain the differences in death rates; other factors such as healthcare quality or pandemic response may play a more important role, but further data is needed to confirm this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
